{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnsight\n",
    "from nnsight import LanguageModel\n",
    "from nnsight.intervention import InterventionProxy\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from rich import print as rprint\n",
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('gpt2')\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 12, Resid stream dim: 768\n"
     ]
    }
   ],
   "source": [
    "print(f'Layers: {model.config.n_layer}, Resid stream dim: {model.config.n_embd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nnsight.contexts.Runner.Runner at 0x7f7afd2040a0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = model.generate(\"At the funeral she said solemnly:\", max_new_tokens=20)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOTE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_kwargs = {\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.3,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "}\n",
    "\n",
    "def calculate_and_apply_steering_vector(\n",
    "    model: LanguageModel,\n",
    "    prompt: str,\n",
    "    activation_additions: List[Tuple[int, float, str]],\n",
    "    n_tokens: int,\n",
    "    n_comparisons: int = 1,\n",
    "    use_bos: bool = True,\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    '''\n",
    "    Performs the steering vector experiments described in the LessWrong post.\n",
    "\n",
    "    Args:\n",
    "        prompt: str\n",
    "            The original prompt, which we'll be doing activation steering on.\n",
    "\n",
    "        activation_additions: List[Tuple[int, float, str]], each tuple contains:\n",
    "            layer - the layer we're applying these steering vectors to\n",
    "            coefficient - the value we're multiplying it by\n",
    "            prompt - the prompt we're inputting\n",
    "            e.g. activation_additions[0] = [6, 5.0, \" Love\"] means we add the \" Love\" vector at layer 6, scaled by 5x\n",
    "\n",
    "        n_tokens: int\n",
    "            Number of tokens which will be generated for each completion\n",
    "\n",
    "        n_comparisons: int\n",
    "            Number of sequences generated in this function (i.e. we generate `n_comparisons` which are unsteered, and\n",
    "            the same number which are steered).\n",
    "\n",
    "    Returns:\n",
    "        unsteered_completions: List[str]\n",
    "            List of length `n_comparisons`, containing all the unsteered completions.\n",
    "\n",
    "        steered_completions: List[str]\n",
    "            List of length `n_comparisons`, containing all the steered completions.\n",
    "    '''\n",
    "    # Add the BOS token manually, if we're including it\n",
    "    if use_bos:\n",
    "        bos = model.tokenizer.bos_token\n",
    "        prompt = bos + prompt\n",
    "        activation_additions = [[layer, coeff, bos + p] for layer, coeff, p in activation_additions]\n",
    "\n",
    "    # Get the (layers, coeffs, prompts) in an easier form to use, also calculate the prompt lengths & check they're all the same\n",
    "    act_add_layers, act_add_coeffs, act_add_prompts = zip(*activation_additions)\n",
    "    act_add_seq_lens = [len(tokenizer.tokenize(p)) for p in act_add_prompts]\n",
    "    assert len(set(act_add_seq_lens)) == 1, \"All activation addition prompts must be the same length.\"\n",
    "    assert act_add_seq_lens[0] <= len(tokenizer.tokenize(prompt)), \"All act_add prompts should be shorter than original prompt.\"\n",
    "\n",
    "    # Get the prompts we'll intervene on (unsteered and steered)\n",
    "    steered_prompts = [prompt for _ in range(n_comparisons)]\n",
    "    unsteered_prompts = [prompt for _ in range(n_comparisons)]\n",
    "\n",
    "    with model.generate(max_new_tokens=n_tokens, remote=False, remote_include_output=True, **sampling_kwargs) as generator:\n",
    "\n",
    "        # Run the act_add prompts (i.e. the contrast pairs), and extract their activations\n",
    "        with generator.invoke(act_add_prompts) as invoker:\n",
    "            # Get all the prompts from the activation additions, and put them in a list\n",
    "            # (note, we slice from the end of the sequence because of left-padding)\n",
    "            act_add_vectors = [\n",
    "                model.transformer.h[layer].output[0][i, -seq_len:]\n",
    "                for i, (layer, seq_len) in enumerate(zip(act_add_layers, act_add_seq_lens))\n",
    "            ]\n",
    "\n",
    "        # Forward pass on unsteered prompts (no intervention, no activations saved - we only need the completions)\n",
    "        with generator.invoke(unsteered_prompts) as invoker:\n",
    "            pass\n",
    "\n",
    "        # Forward pass on steered prompts (we add in the results from the act_add prompts)\n",
    "        with generator.invoke(steered_prompts) as invoker:\n",
    "            # For each act_add prompt, add the vector to residual stream, at the start of the sequence\n",
    "            for i, (layer, coeff, seq_len) in enumerate(zip(act_add_layers, act_add_coeffs, act_add_seq_lens)):\n",
    "                model.transformer.h[layer].output[0][:, :seq_len] += act_add_vectors[i] * coeff\n",
    "\n",
    "    # Decode steered & unsteered completions (discarding the sequences we only used for extracting activations) & return results\n",
    "    unsteered_completions = tokenizer.batch_decode(generator.output[-2*n_comparisons: -n_comparisons])\n",
    "    steered_completions = tokenizer.batch_decode(generator.output[-n_comparisons:])\n",
    "    return unsteered_completions, steered_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Completions                                                    </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Unsteered                                              </span>┃<span style=\"font-weight: bold\"> Steered                                                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ I went up to my friend and said, 'Hey I've got a new   │ I went up to my friend and said, 'You know what? I'm   │\n",
       "│ book coming out. What do you think?' He was like,      │ going back in time.' And he was like, 'Yeah. That's    │\n",
       "│ 'Well it's really good.'                               │ how it goes.'\"                                         │\n",
       "│ \"He told me he'd read the first two books in his life  │  (The cookie-cutter cookies were the Cookie of The     │\n",
       "│ so that would be an interesting thing                  │ Year.) \"He told me that his first thought              │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ I went up to my friend and said, 'What's wrong with    │ I went up to my friend and said, 'What are you talking │\n",
       "│ you?' And he was like, 'Oh no. I'm not going back.' \"  │ about?' He was like, 'You know what cookie I'm talkin' │\n",
       "│  (Photo: AP) Story Highlights A former student at the  │ on.' And he's got a real good understanding of         │\n",
       "│ University of Wisconsin-Madison says she is now a      │ cookies. So we were just kinda sitting there thinking  │\n",
       "│ victim in her                                          │ that maybe it would be better                          │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ I went up to my friend and said, 'Hey buddy. I'm going │ I went up to my friend and said, 'You know what? I'm   │\n",
       "│ over there with you.' And he was like, 'Yeah? What's   │ going to go out with you.' And he was like, 'No.       │\n",
       "│ that?' He had a lot of fun talking about it.\"          │ You're not.'\"                                          │\n",
       "│  (Photo: Courtesy) Story Highlights The first time the │ - The Cookie Monster's cookie is a candy that can be   │\n",
       "│ couple met in                                          │ used as an ice cream or for the cookies of             │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Completions                                                    \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mUnsteered                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSteered                                               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ I went up to my friend and said, 'Hey I've got a new   │ I went up to my friend and said, 'You know what? I'm   │\n",
       "│ book coming out. What do you think?' He was like,      │ going back in time.' And he was like, 'Yeah. That's    │\n",
       "│ 'Well it's really good.'                               │ how it goes.'\"                                         │\n",
       "│ \"He told me he'd read the first two books in his life  │  (The cookie-cutter cookies were the Cookie of The     │\n",
       "│ so that would be an interesting thing                  │ Year.) \"He told me that his first thought              │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ I went up to my friend and said, 'What's wrong with    │ I went up to my friend and said, 'What are you talking │\n",
       "│ you?' And he was like, 'Oh no. I'm not going back.' \"  │ about?' He was like, 'You know what cookie I'm talkin' │\n",
       "│  (Photo: AP) Story Highlights A former student at the  │ on.' And he's got a real good understanding of         │\n",
       "│ University of Wisconsin-Madison says she is now a      │ cookies. So we were just kinda sitting there thinking  │\n",
       "│ victim in her                                          │ that maybe it would be better                          │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ I went up to my friend and said, 'Hey buddy. I'm going │ I went up to my friend and said, 'You know what? I'm   │\n",
       "│ over there with you.' And he was like, 'Yeah? What's   │ going to go out with you.' And he was like, 'No.       │\n",
       "│ that?' He had a lot of fun talking about it.\"          │ You're not.'\"                                          │\n",
       "│  (Photo: Courtesy) Story Highlights The first time the │ - The Cookie Monster's cookie is a candy that can be   │\n",
       "│ couple met in                                          │ used as an ice cream or for the cookies of             │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n",
    "    model,\n",
    "    prompt = \"I went up to my friend and said\",\n",
    "    activation_additions = [\n",
    "        (10, +6.0, \"I talk about cookies constantly  \"),\n",
    "        (10, -6.0, \"I do not talk about cookies constantly\"),\n",
    "    ],\n",
    "    n_tokens = 50,\n",
    "    n_comparisons = 3,\n",
    "    use_bos = False,\n",
    ")\n",
    "\n",
    "table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n",
    "for usc, sc in zip(unsteered_completions, steered_completions):\n",
    "    table.add_row(usc, sc)\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                    Completions                                                    </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Unsteered                                              </span>┃<span style=\"font-weight: bold\"> Steered                                                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ &lt;|endoftext|&gt;I hate you because I'm a racist.          │ &lt;|endoftext|&gt;I hate you because I love my life.        │\n",
       "│ It's not that they're bad people, it's just how we are │                                                        │\n",
       "│ and what our culture is about them being so good at    │ It's been a long time since we've had the chance to    │\n",
       "│ hating us as well. We have to stop doing this shit for │ talk about our lives and what it means to be human,    │\n",
       "│ ourselves or the world around                          │ but now that she has spoken out against her husband    │\n",
       "│                                                        │ for not being able take care of his                    │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ &lt;|endoftext|&gt;I hate you because I love your guts.      │ &lt;|endoftext|&gt;I hate you because I'm a man.             │\n",
       "│                                                        │                                                        │\n",
       "│ And that's why, in the end of my life and for many     │ But that's not what this is about. This is an argument │\n",
       "│ years after it happened, we were all so happy to have  │ for the right to be free from oppression and           │\n",
       "│ him back on our side as a friend who was just about    │ exploitation, which means we have freedom of speech on │\n",
       "│ ready enough to get out                                │ all sides. We can say whatever we want in our          │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ &lt;|endoftext|&gt;I hate you because I don't like your      │ &lt;|endoftext|&gt;I hate you because I love your work.      │\n",
       "│ politics.                                              │                                                        │\n",
       "│                                                        │ But when it comes to the art of making money, this is  │\n",
       "│ But that's not the point of this article. It is a      │ what makes me happy: my passion for music and its      │\n",
       "│ simple question: Why do we need to have more people in │ effects on people's lives has been a source that keeps │\n",
       "│ our country? The answer lies somewhere between         │ coming back into focus as time goes by                 │\n",
       "│ political correctness and \"political Islamophobia.\"    │                                                        │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                    Completions                                                    \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mUnsteered                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSteered                                               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ <|endoftext|>I hate you because I'm a racist.          │ <|endoftext|>I hate you because I love my life.        │\n",
       "│ It's not that they're bad people, it's just how we are │                                                        │\n",
       "│ and what our culture is about them being so good at    │ It's been a long time since we've had the chance to    │\n",
       "│ hating us as well. We have to stop doing this shit for │ talk about our lives and what it means to be human,    │\n",
       "│ ourselves or the world around                          │ but now that she has spoken out against her husband    │\n",
       "│                                                        │ for not being able take care of his                    │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ <|endoftext|>I hate you because I love your guts.      │ <|endoftext|>I hate you because I'm a man.             │\n",
       "│                                                        │                                                        │\n",
       "│ And that's why, in the end of my life and for many     │ But that's not what this is about. This is an argument │\n",
       "│ years after it happened, we were all so happy to have  │ for the right to be free from oppression and           │\n",
       "│ him back on our side as a friend who was just about    │ exploitation, which means we have freedom of speech on │\n",
       "│ ready enough to get out                                │ all sides. We can say whatever we want in our          │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ <|endoftext|>I hate you because I don't like your      │ <|endoftext|>I hate you because I love your work.      │\n",
       "│ politics.                                              │                                                        │\n",
       "│                                                        │ But when it comes to the art of making money, this is  │\n",
       "│ But that's not the point of this article. It is a      │ what makes me happy: my passion for music and its      │\n",
       "│ simple question: Why do we need to have more people in │ effects on people's lives has been a source that keeps │\n",
       "│ our country? The answer lies somewhere between         │ coming back into focus as time goes by                 │\n",
       "│ political correctness and \"political Islamophobia.\"    │                                                        │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n",
    "    model,\n",
    "    prompt = \"I hate you because\",\n",
    "    activation_additions = [\n",
    "        (11, +5.0, \"Love \"),\n",
    "        (11, -5.0, \"Hate\"),\n",
    "    ],\n",
    "    n_tokens = 50,\n",
    "    n_comparisons = 3,\n",
    "    use_bos = True,\n",
    ")\n",
    "\n",
    "table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n",
    "for usc, sc in zip(unsteered_completions, steered_completions):\n",
    "    table.add_row(usc, sc)\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "unsteered_completions, steered_completions = calculate_and_apply_steering_vector(\n",
    "    model,\n",
    "    prompt = \"To see the eiffel tower, people flock to\",\n",
    "    activation_additions = [\n",
    "        (9, +10.0, \"The Eiffel Tower is in Rome\"),\n",
    "        (9, -10.0, \"The Eiffel Tower is in France\"),\n",
    "    ],\n",
    "    n_tokens = 50,\n",
    "    n_comparisons = 3,\n",
    "    use_bos = False,\n",
    ")\n",
    "\n",
    "table = Table(\"Unsteered\", \"Steered\", title=\"Completions\", show_lines=True)\n",
    "for usc, sc in zip(unsteered_completions, steered_completions):\n",
    "    table.add_row(usc, sc)\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autosteering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
